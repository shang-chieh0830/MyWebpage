---
title: "Linear Regression"
author: "Jay Wei"
date: "2023-03-19"
output: html_document
---

## load packages
```{r}
library(tidyverse, quietly = TRUE)
```

## load data, coerce to tibble
```{r}
# Instead of loading the whole package ISLR, we only want the data Credit
# This is why we use ISLR::Credit
crdt <- as_tibble(ISLR::Credit)
```

## data prep
```{r}
# The ID col is not needed, we can delete it
crdt <- crdt %>%
  select(-ID)
```

## test-train split data
```{r}
set.seed(1)
# randomly choose 0.8*nrow(crdt) rows from crdt
trn_idx <- sample(nrow(crdt), size=0.8*nrow(crdt))

trn <- crdt[trn_idx,]
tst <- crdt[-trn_idx,]
```

## estimation-validation split data
```{r}
est_idx <- sample(nrow(trn), size=0.8*nrow(trn))
est <- trn[est_idx,]
val <- trn[-est_idx,]
```

## look at the data
```{r}
str(trn)
```

```{r}
View(trn)
```

```{r}
skimr::skim(trn)
```

```{r}
pairs(trn)
```

```{r}
GGally::ggpairs(trn)
```

## fit candidate modesls
```{r}
# intercept-only model
mod_1 <- lm(Rating~1, data=est) 
mod_2 <- lm(Rating~ Balance, data=est)
mod_3 <- lm(Rating~ Balance+Income+Limit, data=est)
mod_4 <- lm(Rating~., data=est)
mod_5 <- step(lm(Rating~. ^2, data=est), trace=FALSE)


summary(mod_1)
```

```{r}
mean(est$Rating)
coef(mod_1)
```

```{r}
predict(mod_1, val)
# make predictions on validation data
# note that validation data only have 64 obs
```

```{r}
predict(mod_1, newdata=val)
```


```{r}
# wrong code
predict(mod_1, data=val)
# we want to make predictions on validation data(64 obs)
# but below it show 256 obs
# you should spciefy the 2nd argument to be newdata, o.w., R will predict on the data that you fit the model by default
```

```{r}
predict(mod_1, est)
```

```{r}
predict(mod_1)
```

Don't name the 2nd argument in `predict()` for your own sake!!

## Calculate RMSE
```{r}
sqrt(mean((val$Rating-predict(mod_1, val))^2))
```

```{r}
calc_rmse <- function(actual, predicted){
  sqrt(mean((actual-predicted)^2))
}

calc_rmse(actual=val$Rating, predict(mod_1, val))
calc_rmse(actual=val$Rating, predict(mod_2, val))
calc_rmse(actual=val$Rating, predict(mod_3, val))
calc_rmse(actual=val$Rating, predict(mod_4, val))
calc_rmse(actual=val$Rating, predict(mod_5, val))

# validation rmse is used to select a model
# it seems that mod_4 is the best
```
## fit a final model
```{r}
mod_final <- lm(Rating~., data=trn)
```

## calculate test RMSE
```{r}
calc_rmse(actual = tst$Rating, predicted = predict(mod_final, tst))
```

## plot actual vs. predicted 
```{r}
plot(x= tst$Rating, y=predict(mod_final, tst),
     pch=20, col="darkgrey",
     main="Credit: Predicted vs. Actual, Test Data",
     xlab= "Actual", ylab="Predicted")

abline(a=0, b=1, lwd=2)
grid()
```

## Don't repeat yourself


```{r}
mod_list = list(
  mod_1 <- lm(Rating~1, data=est), 
  mod_2 <- lm(Rating~ Balance, data=est),
  mod_3 <- lm(Rating~ Balance+Income+Limit, data=est),
  mod_4 <- lm(Rating~., data=est),
  mod_5 <- step(lm(Rating~. ^2, data=est), trace=FALSE)
)
```

```{r}
mod_list[3]
```

```{r}
pred_val <- lapply(mod_list, predict, val)

```


```{r}
# This is what lapply just did

pred_val_silly_way <- list(
  predict(mod_1, val),
  predict(mod_2, val),
  predict(mod_3, val),
  predict(mod_4, val),
  predict(mod_5, val)
)
```

```{r}
lapply(pred_val, calc_rmse, actual=val$Rating)
# lapply always gives you lists
```

```{r}
sapply(pred_val, calc_rmse, actual= val$Rating)
# s stands for simplifed
```

